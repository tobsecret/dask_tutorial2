{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "#Imports for our little html parser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bioservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure\n",
    "This notebook will go over three parts of dask. Reflected in each part is my experience with dask.\n",
    "- **dask basics**: dask.delayed, dask.compute, dask schedulers\n",
    "- **dask.dataframe**: Caveats & work-arounds, meta keyword\n",
    "- **dask concurrent-futures interface**: Basic useage \n",
    "\n",
    "## Dask basics\n",
    "Dask is a library that makes paralellizing computation in python easy.\n",
    "At its core dask really just has three important parts that you need to understand when working with it. \n",
    "\n",
    "### dask.delayed\n",
    "Dask splits parallelization into two components: \n",
    "- constructing a task graph\n",
    "- scheduling the individual tasks of the graph\n",
    "\n",
    "`dask.delayed` enables what is called **lazy computation**. Lazy computation means that the time at which you define how your computation should take place, and the time the computation is performed are not the same. This means you can build what is called a **task graph** that defines which functions should be executed in which order. This is exactly what `dask.delayed` is meant for. \n",
    "\n",
    "It \"remembers\" a function and the specified inputs. `dask.delayed` can be used to wrap functions and classes to make their execution \"lazy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstsum = add(add(1, 2), 3) # = 1 + 2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the task graph we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsum.visualize() #This requires the system graphviz package and the python-graphviz package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task graph above all the circles represent the functions and the rectangles represent the outputs of those functions.\n",
    "\n",
    "Now a `Delayed` object by itself is not very useful, since it is just a task graph, no actual computation has taken place yet.\n",
    "\n",
    "To get the actual result, simply call the compute method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task graph above was linear, so there is not much to parallelize. What if we have a bunch of task graphs like the one above though, how can we parallelize the computation of all of them?\n",
    "\n",
    "The answer is `dask.compute` which any number of python collections (e.g. a list) of Delayed objects, or individual Delayed objects and avoids multiple computation of the same sub-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "@delayed\n",
    "def slow_ass_function(*args, **kwargs):\n",
    "    sleep(1) #sleeps for 1 second to simulate expensive computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slow_ass_function().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(*[slow_ass_function(slow_ass_function()) for _ in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dask.compute([slow_ass_function(slow_ass_function()) for _ in range(2)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "[slow_ass_function(slow_ass_function()).compute() for _ in range(2)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `dask.compute` instead of calling the compute method of a bunch of `Delayed` objects can save time when the task graphs are partially redundant. Note that `dask.compute` can only optimize task graphs from the bottom up, meaning that if the redundant part of the graph is not at the leaf, it cannot optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@delayed\n",
    "def one_plus_two(*args):\n",
    "    return 1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(add(one_plus_two(add(1,2)), one_plus_two(3)), add(add(1, one_plus_two(3)), one_plus_two(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dask.compute cannot optimize the graph on the right even though one_plus_two is called twice here and its output does not depend on its input in this case. It can however optimize the graph on the left.\n",
    "\n",
    "This is because `dask.optimize`, the function that does the task graph optimization under the hood, basically starts from the leafs (nodes that have only one connection) of the task graph and takes in nodes one-by-one until none of the subgraphs it finds are the same anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  dask schedulers\n",
    "Constructing a dask task graph is nice and dandy but somebody has got to do the work. This is where the other core aspect of dask, the scheduler becomes important. A **scheduler** decides which task is executed where and at what time. \n",
    "\n",
    "Parallel computing is also possible in python without the use of dask, by using the multiprocessing module from the standard library. What really sets dask apart from multiprocessing are its schedulers. With multiprocessing you have to decide which task is executed where and at what time, dask lifts that burden from you. \n",
    "\n",
    "There is however still some thought required when using dask, choosing the right scheduler for the execution of a computation graph can make a big difference in performance.\n",
    "\n",
    "In a python process only one thread can execute python code a time, however all threads within each process have access to all the data inside that process.\n",
    "\n",
    "#### threaded scheduler (`dask.threaded.get`)\n",
    "The threaded scheduler is good when sharing data between different tasks is important and the computations that are happening are not actually that expensive or achieve parallelism by themselves (numpy, pandas, sklearn, numba).\n",
    "\n",
    "#### multiprocessing scheduler (`dask.multiprocessing.get`)\n",
    "The multiprocessing scheduler is good when each task can be run by itself and does not have to communicate large intermediate results like big matrices a lot. This scheduler is really good when you have a huge number of tasks each of which only communicate a small number of \n",
    "\n",
    "#### synchronous scheduler  (`dask.get`)\n",
    "The synchronous scheduler as the name suggests, runs tasks one after the other, so achieves no parallelism at all. This makes debugging easier however, which is its intended use.\n",
    "\n",
    "#### distributed scheduler  (`distributed.Client.get`)\n",
    "First and foremost, the distributed scheduler allows for computation spread across multiple machines.\n",
    "\n",
    "On a single machine it is often a good alternative to the multiprocessing scheduler because it can make some smart decisions when dealing with task graphs. When your task graph has a huge number of small tasks, this scheduler is a poor choice however because each task has a significant scheduling overhead of about 0.2 Âµs, in which the multiprocessing scheduler is better suited.\n",
    "\n",
    "Lastly the distributed scheduler also allows for asynchronous computing, more about that later in this notebook.\n",
    "\n",
    "more information can be found [here](http://dask.pydata.org/en/latest/scheduler-choice.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dask.dataframe\n",
    "Another strength of dask in scientific computing is that it comes ready with parallel implementations of pandas.DataFrame (`dask.dataframe`) and numpy arrays (`dask.array`).\n",
    "In the following I will go over some caveats of the use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat #1: No assignment\n",
    "`dask.dataframe` does not support assignment to columns or rows. Once a dataframe is created, its columns and rows cannot be replaced because not the entire dataframe is in memory, hence every assignment would have to be written to disk.\n",
    "\n",
    "Work-around: Use map_partitions\n",
    "\n",
    "--> this applies a function to each partition of the dataframe. Each partition is a pandas DataFrame.\n",
    "\n",
    "--> We just have the function we apply to each partition simply return a new dataframe with our desired proportions.\n",
    "\n",
    "--> We have to supply the **meta** keyword to tell dask the output format. Dask expects either a pandas Series or a pandas DataFrame for each partition.\n",
    "\n",
    "--> Finally we probably want to write to file(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just take the simple task of correcting the names in the `chr` column of a gtf (General Transfer Format, a genome annotation file) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading in the file; notice that we supply dtypes for some columns.\n",
    "#This is because the chr column has some numbers and then later some strings, so dask gets confused because some\n",
    "#rows have an int and some have a str.\n",
    "gtf = dd.read_table('TB_human_sorted.gtf', dtype={'chr':'str', 'info':'str'}, \n",
    "                    names=['chr', 'origin','type', 'start', 'stop', 'noidea1', 'strand', 'noidea2', 'info'], \n",
    "                    comment='#', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf.chr.unique().compute() # to get much of anything with dask.dataframe, we usually have to call compute. \n",
    "                           #Exceptions are head and any writing to file method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gtf['chr'] = 'chr' + gtf.chr.compute() #This will throw an error because we cannot assign in dask the way we would in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that returns the original DataFrame but exchanges corrects the values in the chr column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_chr(df):\n",
    "    return pd.merge(\n",
    "        pd.DataFrame(\n",
    "            df['chr'].apply(lambda cell: gtf_sam_dict[cell])\n",
    "        ), \n",
    "        df.iloc[:,1:], \n",
    "        left_index=True, \n",
    "        right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition1 = gtf.get_partition(0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./gtf_sam_dict.json', 'r') as fp:\n",
    "    gtf_sam_dict = json.load(fp) #this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrected = gtf.map_partitions(correct_chr, \n",
    "                   meta=partition1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected.chr.unique().compute() #yay, our computations were all finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Caveat #2: Cannot write to one file\n",
    "`dask.dataframe` does not allow writing into a single file, it does however support writing to multiple files.\n",
    "Work-around: if you want to write to a single file, in bash you can use cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will throw an error\n",
    "corrected.to_csv('TB_human_sorted_corrected.gtf', sep='\\t', index=False, header=None, \n",
    "                 quoting=csv.QUOTE_NONE #This line ensures that Pandas does not end up \n",
    "                                        #putting quotation marks around any line with white spaces in it.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this would pass\n",
    "#corrected.to_csv('TB_human_sorted_corrected*.gtf', sep='\\t', index=False, header=None, quoting=csv.QUOTE_NONE #This line ensures that Pandas does not end up \n",
    "                                        #putting quotation marks around any line with white spaces in it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent Futures with dask\n",
    "In python concurrent futures allow asynchronous execution of code, meaning that not only does the code run in parallel, subsequent python code can still be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = pd.read_csv('./UniProtIDs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gene_from_uniprot(url, sleepy=False):\n",
    "    ''' \n",
    "    Takes a uniprot URL and gets the gene name field.\n",
    "    '''\n",
    "    if sleepy:\n",
    "        sleep(25)\n",
    "    if url:\n",
    "        try:\n",
    "            return str(\n",
    "                next(\n",
    "                    next(\n",
    "                        BeautifulSoup(\n",
    "                            requests.get(url).content, 'html.parser')\\\n",
    "                        .find('div',{'id':'content-gene'}).children\n",
    "                    ).children)\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "future = c.submit(get_gene_from_uniprot, prots.UniProtID.iloc[0], sleepy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#future.result() will force the computation for future to complete and wait for its result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But computing just one future is no fun. The real fun begins when you can just have a ton of futures compute.\n",
    "\n",
    "The **progress** function from `dask.distributed` can show you real-time progress of your work, and by clicking the dashboard link of your `dask.distributed.Client` you can monitor the work being done on your cores in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = [c.submit(get_gene_from_uniprot, url) for url in prots.UniProtID]\n",
    "progress(genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how you can still execute python code in this jupyter notebook even though your cores are hard at work on your concurrent futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots['genes'] = pd.Series([gene.result() for gene in genes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
